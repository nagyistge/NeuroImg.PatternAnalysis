{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2: decoding analyses\n",
    "\n",
    "This week's tutorial is about how to implement decoding analyses in Python. We'll be looking at the implementation of concepts like K-fold cross-validation, feature-selection/extraction, model fitting/prediction, permutation testing, and feature visualization. To do so, we'll use the [`scikit-learn`](http://scikit-learn.org) machine learning package in Python (the de-facto and most-used ML package in Python). Additionally, we'll use some functionality from the [`skbold`](http://skbold.readthedocs.io/en/latest/) to simplify some of steps in implementing decoding pipelines. In this tutorial, we'll use the PIOP-dataset (**P**opulation **I**maging **O**f **P**sychology), a large-scale (>240 subjects) multimodal MRI-dataset, including structural (gray-matter [VBM](https://en.wikipedia.org/wiki/Voxel-based_morphometry) and white-matter [TBSS](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS)), resting-state (incl. [dual regression](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/DualRegression) data), and task-based functional data. Along with this set of MRI-scans, the PIOP-study also acquired a set of behavioral (psychometric) variables from various questionnaires and behavioral task (outside the scanner) of the same participants who were scanned. As such, this makes it an excellent data-set for decoding analyses in which you can use neural patterns (from any MRI modality; `X`) to predict a behavioral variable (`y`). This is, by the way, also one of the data-sets that you can use for your final project (more info on Blackboard/week_4).\n",
    "\n",
    "For this tutorial, we'll focus on a specific decoding analysis in which *we try to predict intelligence (y) from brain patterns (X) associated with working memory processing* (based on the same data as you practiced with in the tutorial of week 1). Throughout the tutorial, we'll discuss all the steps necessary to answer this research question. (Mini-ToThink: is this a within- or between-subject analysis?)\n",
    "\n",
    "In terms of skills, after this tutorial you are be able to:\n",
    "\n",
    "* Load and appropriately transform dependent variables (`y`) corresponding to your patterns (`X`);\n",
    "* Apply basic [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) to your patterns;\n",
    "* Implement [feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) and extraction processes;\n",
    "* Build fully cross-validated [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) using scikit-learn;\n",
    "* Run appropriate statistics (including permutation tests) on your model performance scores;\n",
    "* Visualize features by \"back-projecting\" (inverted) features to brain-space;\n",
    "\n",
    "In combination with the knowledge/skills you acquired from last week's tutorial, you'll be able to implement a complete decoding pipeline yourself at the end of this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data representation (yet again)\n",
    "Before we'll start with the machine learning stuff, we have to load in our patterns again. Like last week, you are going to use a custom `Mvp` class for this again, which we'll extend throughout this tutorial. You may use your own class from last week (given that it contains correct implementations of the load, standardize and apply_mask methods), or use the one in the solutions-notebook from last week. In any case, copy the code for the Mvp-class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "class Mvp():\n",
    "    \"\"\" Custom class to load, organize, and process multivoxel MRI patterns. \"\"\"\n",
    "    \n",
    "    def __init__(self, paths):\n",
    "        \n",
    "        self.paths = paths\n",
    "        \n",
    "    def load(self, voxel_dims=(91, 109, 91)):\n",
    "        \n",
    "        X = np.zeros((len(self.paths), np.prod(voxel_dims)))\n",
    "\n",
    "        # Start your loop here!\n",
    "        for i, path in enumerate(self.paths):\n",
    "    \n",
    "            X[i, :] = nib.load(path).get_data().ravel()\n",
    "        \n",
    "        self.X = X\n",
    "    \n",
    "    def standardize(self):\n",
    "        self.X = (self.X - self.X.mean(axis=0)) / self.X.std(axis=0)\n",
    "        self.X[np.isnan(self.X)] = 0\n",
    "        \n",
    "    def apply_mask(self, path_to_mask, threshold):\n",
    "        \n",
    "        mask = nib.load(path_to_mask).get_data()\n",
    "        mask_bool = mask > threshold\n",
    "        self.X = self.X[:, mask_bool.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given our research question, we're going to use the (between-subject!\\*) patterns of the working-memory task again. More specifically, we are going to use the patterns from the \"active-passive\" contrast of each subject (just like last week).\n",
    "\n",
    "---------------\n",
    "\\* It's a between-subject analysis because intelligence is a factor that varies at the subject level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to do now is to load in the tstat3 patterns from all subjects who completed the working-memory task in the scanner. We put this dataset in the `/home/Public/FirstLevel_piop` folder. This folder (and all its subdirectories) is accessible (read-only, so you cannot modify anything) for all NIPA-students. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Click on the \"Files\" icon on your desktop and navigate (just by clicking, not in the terminal) to this directory (note: you have to go up one directory relative to your `home` directory).<br><br>\n",
    "\n",
    "Check out the structure. You should see a bunch of subdirectories with subject-names (e.g. pi0010, pi0204, pi0084). These directories contain the first-level data (.feat directories) from the different task-based MRI-runs and some separate nifti-files (we'll get to these later). As you can see, we already estimated the (between-subject) patterns for you (which can be found in the .feat directories)! Go to a random subject's directory and click on the `*piopwm.feat` directory (note that not *all* subjects have this directory, because some subjects did not complete this run in the scanner due to technical issues or time issues). Check out the `design.png` file and the `reg_standard` directory (which files are in there? Do you understand what each file represents?). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is structured in an hierarchical way:\n",
    "\n",
    "`/home/Public/FirstLevel_piop/{subject-no}/{task}.feat/reg_standard/{tstat-files}.nii.gz`\n",
    "\n",
    "Given this structure, it's easy to `glob` a set of tstat-files for a specific task across all subjects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: use glob to find all the paths to the tstat3-files of the piopwm task. (Hint 1, you will need *two* wildcards in your search-string; Hint 2, you should find 217 paths in total)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# Implement the todo here (insert the right search-query in glob()):\n",
    "from glob import glob\n",
    "paths = glob('/home/lukas/between_data/pi*/*piopwm*/reg_standard/tstat3.nii.gz')\n",
    "#paths = glob('/media/lukas/piop/PIOP/FirstLevel_piop/pi*/*piopwm*/reg_standard/tstat3.nii.gz')\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the paths, such that the paths are sorted by subject-number (pi0010 to pi0248). Also, let's not use *all* data (all subjects) at once, because that's actually quite memory-intensive and we don't want the TUX-server to crash. Therefore, we'll select only every third path (i.e. path 1, 4, 7, 10, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths was: 29\n",
      "Number of paths is now: 10\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(paths)\n",
    "print(\"Number of paths was: %i\" % len(paths))\n",
    "subset_paths = paths[1::3]\n",
    "print(\"Number of paths is now: %i\" % len(subset_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have 72 paths (sorted by subject-number) to the subjects' tstat3 files. Let's initialize the Mvp object and call the `load()` method! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: before you initialize the Mvp object and load the patterns, can you predict what shape the `X`-attribute of the Mvp-object will be? (Hint: images in MNI152 2mm space are of shape 91\\*109\\*91)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, less thinking, more doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mvp = Mvp(paths=subset_paths)\n",
    "mvp.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the `X`-attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is: (10, 902629)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X is: %s\" % (mvp.X.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about standardization\n",
    "We briefly touched upon the issue of \"standardization\" last week, but the process of standardization is especially important in decoding analyses, so it warrants some extra explanation. \n",
    "\n",
    "As explained before, standardization refers to the process of converting numbers in a vector to the (signed) number of standard deviations it deviates from the mean of the vector. Formally, for a vector `x` with mean $\\bar{x}$ and standard deviation $s_{x}$ and length `N`, the standardized value of element $x_{i}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "x_{i} = \\frac{x_{i} - \\bar{x}}{s_{x}}\n",
    "\\end{align}\n",
    "\n",
    "Practically, standardization in machine learning is applied to each column (i.e. feature), ensuring that each feature (in decoding: voxel) is on the same scale. In other words, standardization ensures that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "\"But *why* do you have to standardize your features?\", you might reasonably ask. Well, that has to do with the way many ML models calculate their parameters (i.e. $\\beta$; this process is called 'gradient descent'). For most models, this process may fail to converge and thus is unable to calculate the model's parameters is not all features are on the same scale. \n",
    "\n",
    "Anyway, just remember that *before you do any model fitting* your features should be standardized\\*. Fortunately, you already implemented this yourself last week as a method in your mvp-class: the `standardize()` method! So, before we go on, let's just standardize our features:\n",
    "\n",
    "-----------\n",
    "\\* There are also other ways to get your features on the same scale, like min-max scaling, but standardization is by far the most often used method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "mvp.standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we got the (standardized) patterns (`X`) that we wanted. But as you might have noticed, we're still missing one crucial component for our decoding analysis: `y` (our to-be-decoded feature)! For our research question, we chose to focus on decoding intelligence, but how do we load and represent this in our pipeline? This is what the next section is about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding a dependent variable (`y`)\n",
    "While there is kind of a generic way to load in voxel patterns, there is usually not a single way to load in your dependent variable (y), because the exact factor that represents `y` dependent on your exact research question (and also depends how you have stored this data on disk). \n",
    "\n",
    "We'll focus on extracting a DV from a common tsv-file (like a csv-file, but not with comma- but tab-delimited values). Before we do that, just open the file and take a look at its structure. Just go to your desktop, double-click \"Files\", go up one directory (to `/home`) and go to `/home/Public/FirstLevel_piop`. Scroll aaaall the way down (past all the pi\\* folders) and you'll find a file called `PIOP_BEHAV_NIPA.tsv` file. Double-click this, and press \"Display\". A \"Text Import\" window will pop up - select here \"Tab\" under the \"Separated by\" option. Then, click \"OK\". \n",
    "\n",
    "You'll see a spreadsheet with the subject-names in the first column and different variables in the columns. Basically, you can choose any variable in the columns as your to-be-decoded factor! For our example, we will pick the 'Raven' variable, which represents the subjects' (continuous) scores on the [Raven](https://en.wikipedia.org/wiki/Raven%27s_Progressive_Matrices) test, which aims to measure fluid intelligence. \n",
    "\n",
    "### 2.1. Loading behavioral data\n",
    "To load in the tsv-file, we'll use `pandas`, a Python package for working with tabular data. It's main data structure is called the `DataFrame`, which is very similar to the dataframe-object in R. (Working with pandas is beyond the scope of this course, but you'll see it now and then throughout the course when we work with tsv-files.)\n",
    "\n",
    "We'll walk you through how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEO_A</th>\n",
       "      <th>NEO_C</th>\n",
       "      <th>NEO_E</th>\n",
       "      <th>NEO_N</th>\n",
       "      <th>NEO_O</th>\n",
       "      <th>DOB_day</th>\n",
       "      <th>DOB_month</th>\n",
       "      <th>DOB_year</th>\n",
       "      <th>DOB_dmy</th>\n",
       "      <th>Gender</th>\n",
       "      <th>...</th>\n",
       "      <th>Opl_nu_rcht</th>\n",
       "      <th>Opl_tot.1</th>\n",
       "      <th>Opl_rcht_tot.1</th>\n",
       "      <th>Sexual_preference</th>\n",
       "      <th>Religie</th>\n",
       "      <th>Raven_raw</th>\n",
       "      <th>Raven_zscored</th>\n",
       "      <th>Raven_corrected_for_brainsize</th>\n",
       "      <th>wm_score</th>\n",
       "      <th>brainsize_t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pi0001</th>\n",
       "      <td>47.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>10/02/1994</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.696968</td>\n",
       "      <td>-0.571876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1327159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pi0002</th>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>05/09/1992</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.740011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pi0003</th>\n",
       "      <td>38.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>09/06/1990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.081120</td>\n",
       "      <td>-0.198713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1504618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pi0004</th>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10/27/1991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.286403</td>\n",
       "      <td>-0.172240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1358498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pi0005</th>\n",
       "      <td>42.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>08/25/1992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.081120</td>\n",
       "      <td>-0.217163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1521833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NEO_A  NEO_C  NEO_E  NEO_N  NEO_O  DOB_day  DOB_month  DOB_year  \\\n",
       "pi0001   47.0   39.0   35.0   40.0   42.0      2.0       10.0    1994.0   \n",
       "pi0002   46.0   37.0   43.0   29.0   48.0      9.0        5.0    1992.0   \n",
       "pi0003   38.0   34.0   45.0   36.0   41.0      6.0        9.0    1990.0   \n",
       "pi0004   47.0   46.0   40.0   22.0   43.0     27.0       10.0    1991.0   \n",
       "pi0005   42.0   47.0   49.0   24.0   45.0     25.0        8.0    1992.0   \n",
       "\n",
       "           DOB_dmy  Gender      ...       Opl_nu_rcht  Opl_tot.1  \\\n",
       "pi0001  10/02/1994     2.0      ...               6.0        2.0   \n",
       "pi0002  05/09/1992     2.0      ...               2.0        2.0   \n",
       "pi0003  09/06/1990     1.0      ...               7.0        1.0   \n",
       "pi0004  10/27/1991     2.0      ...              12.0        2.0   \n",
       "pi0005  08/25/1992     1.0      ...              12.0        2.0   \n",
       "\n",
       "        Opl_rcht_tot.1  Sexual_preference  Religie  Raven_raw  Raven_zscored  \\\n",
       "pi0001             6.0                1.0      1.0         21      -0.696968   \n",
       "pi0002             2.0                1.0      2.0         28       0.740011   \n",
       "pi0003             7.0                2.0      1.0         24      -0.081120   \n",
       "pi0004             9.0                1.0      1.0         23      -0.286403   \n",
       "pi0005             9.0                2.0      1.0         24      -0.081120   \n",
       "\n",
       "        Raven_corrected_for_brainsize  wm_score  brainsize_t1  \n",
       "pi0001                      -0.571876       NaN     1327159.0  \n",
       "pi0002                            NaN       NaN           NaN  \n",
       "pi0003                      -0.198713       NaN     1504618.0  \n",
       "pi0004                      -0.172240       NaN     1358498.0  \n",
       "pi0005                      -0.217163       NaN     1521833.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # This is how pandas usually is imported\n",
    "\n",
    "#path_to_tsv = '/media/lukas/piop/PIOP/FirstLevel_piop/PIOP_BEHAV_NIPA.tsv'\n",
    "path_to_tsv = '~/between_data/PIOP_BEHAV_NIPA.tsv'\n",
    "\n",
    "# the read_csv function is the standard way to load in spreadsheet-like data\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', index_col=0)\n",
    "\n",
    "# Let's see how this looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the DataFrame is simply a spreadsheet-like representation of the data, with the subjects represented as different rows and the different behavioral/demographic variables represented as columns. Now, we'd like to extract the `Raven_raw` columns, which represents the raw (summed) scores on the Raven-test for each subject. To do so, we can simply index the `df` variable with the string \"Raven_raw\" (just like indexing a dictionary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y is: 248\n",
      "\n",
      "y looks like:\n",
      "\n",
      "pi0001    21\n",
      "pi0002    28\n",
      "pi0003    24\n",
      "pi0004    23\n",
      "pi0005    24\n",
      "pi0006    31\n",
      "pi0007    25\n",
      "pi0008    25\n",
      "pi0009    23\n",
      "pi0010    18\n",
      "pi0011    26\n",
      "pi0012    23\n",
      "pi0013    20\n",
      "pi0014    28\n",
      "pi0015    23\n",
      "pi0016    25\n",
      "pi0017    28\n",
      "pi0018    14\n",
      "pi0019    25\n",
      "pi0020    19\n",
      "pi0021    32\n",
      "pi0022    21\n",
      "pi0023    19\n",
      "pi0024    26\n",
      "pi0025    27\n",
      "pi0026    24\n",
      "pi0027    14\n",
      "pi0028    30\n",
      "pi0029    26\n",
      "pi0030    24\n",
      "          ..\n",
      "pi0219    17\n",
      "pi0220    14\n",
      "pi0221    20\n",
      "pi0222    25\n",
      "pi0223    21\n",
      "pi0224    28\n",
      "pi0225    20\n",
      "pi0226    25\n",
      "pi0227    30\n",
      "pi0228    24\n",
      "pi0229    14\n",
      "pi0230    19\n",
      "pi0231    20\n",
      "pi0232    20\n",
      "pi0233    26\n",
      "pi0234    26\n",
      "pi0235    23\n",
      "pi0236    16\n",
      "pi0237    32\n",
      "pi0238    28\n",
      "pi0239    20\n",
      "pi0240    23\n",
      "pi0241    23\n",
      "pi0242    24\n",
      "pi0243    25\n",
      "pi0244    25\n",
      "pi0245    23\n",
      "pi0246    32\n",
      "pi0247    24\n",
      "pi0248    23\n",
      "Name: Raven_raw, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['Raven_raw']\n",
    "print(\"Length of y is: %i\\n\" % y.shape[0])\n",
    "print(\"y looks like:\\n\\n%r\" % y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is now a subset of the original dataframe (df) that only contains the Raven scores and a corresponding \"index\", which refers to the subject it belongs to. \n",
    "\n",
    "But we have a problem! \n",
    "\n",
    "`y` now contains 248 values (from 248 subjects), but our X attribute contains patterns from only 70 subjects (the subset selected earlier). For our analysis, the samples in our patterns (X) should match the entries in our dependent variable (y). How do we do this? Well, we need to extract which subjects were included in X and then use those subjects to \"index\" y. \n",
    "\n",
    "Below, we show you a way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pi0061', 'pi0064', 'pi0067', 'pi0070', 'pi0073', 'pi0076', 'pi0079', 'pi0083', 'pi0086', 'pi0089']\n"
     ]
    }
   ],
   "source": [
    "# First we need to extract which subjects were actually included in X.\n",
    "# We do this by extracting this info from the \"subset_paths\" from earlier\n",
    "# using a list-comprehension (a fancy for loop)\n",
    "subject_names = [path.split('/')[4] for path in subset_paths]\n",
    "\n",
    "print(subject_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 902629)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# Here we use the pandas dataframe \"loc\" method, which slices the DataFrame by their\n",
    "# index names (and we subsequently transform the result to a numpy array)\n",
    "y_new = np.array(y.loc[subject_names])\n",
    "\n",
    "# And we can see now that the length of y is the same as the number of samples in X\n",
    "print(mvp.X.shape)\n",
    "print(y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a set of patterns (X) with a corresponding vector representing the to-be-decoded feature (y). There is, however, still one issue worth discussing. At this moment, `y` is a continuous measure (sum-score of performance on the Raven test). It follows then that our decoding analysis would use regression, but we will actually recode `y` such that it is categorical, as explained in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Continuous vs. categorical DVs\n",
    "In some scenario's you might want to recode your continuous dependent variable (y) into a categorical one. One reason is simply because fMRI patterns are incredibly noisy, and classification analyses are more robust than regression analyses. Note, though, that if you recode your continuous DV into a categorical one, your conclusions about your model also become weaker: regression models allow you to explain/model variance across a larger interval than classification models.\n",
    "\n",
    "For example, would you be more impressed if told you I could model predict your IQ with an error of +/- 2 points (a continuous DV) or if I told you I could only predict whether your IQ is higher or lower than 100 (a categorical DV)? Given that your dependent variable is truly continuous, it's better to subject it to a regression analysis in terms of strength of your conclusion, but given the very noisy nature of fMRI data it may be advantageous to trade in some of this inferential strength for some extra SNR by recoding your DV into a categorical variable. \n",
    "\n",
    "This is probably also the reason that *classification* analyses are way more popular in the fMRI community than *regression* analyses. That's why we'll recode the Raven score into a categorical variable and continue with that for the rest of the tutorial. \n",
    "\n",
    "But how should we recode our DV? There are various ways to do this, but we'll use the median-split: any values above the median of `y` will be coded as 1 and any values below the median of `y` will be coded 0. Thus, our updated decoding goal is to predict whether someone has a high intelligence (`y = 1`) or low intelligence (`y = 0`).  \n",
    "\n",
    "We'll show you how to implement this median-split transfor below ... No wait, you know what, by now you guys should be quite the Python experts: try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Transform mvp.y into categorical values {0, 1} using a median-split (you can use the numpy-function `median` to do this, i.e. np.median(some_array)). Hint: maybe you can use the [astype()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html) method of numpy arrays to convert boolean arrays to integer arrays?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform mvp.y into zeros and ones using a median split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, finally, we have everything we need for a decoding analysis: a sample-by-voxel array (X) representing the patterns and a vector with the corresponding to-be-decoded (categorical) variable (y). This is really all we need for a decoding analysis, we promise.\n",
    "\n",
    "But the process of loading in a y-variable, making sure it corresponds to the same subjects contained in X, and transforming it using a median-split is kinda messy. For the following interim assignment, you have to incorporate this process in your Mvp class! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 1** (6 points): In this assigment, you will extend your Mvp class such that it is able to load in a y-variable from a tsv-file and index it such that it contains the data from the same subjects that were included in the X-attribute. So, first copy-paste the code for your class in the code-cell below. Then, do three things:<br><br>\n",
    "\n",
    "1. *Within* your load() method, extract the subject-names from the `paths` attribute and store the results (which should be a list with subject-names, like this: ['pi0010', 'pi0011', 'pi0012', ..., etc.]) in a *new* attribute named 'subject_names'. You can use the list-comprehension defined earlier if you want to extract the subject-names. So, after calling the load() method, your mvp-object should also contain the attribute `subject_names` which represents which subjects the X-attribute is based upon.<br><br>\n",
    "\n",
    "2. Then, add a new method named `load_y` that takes two arguments: a path (a string) to a tsv-file and a variable name (a string) referring to the column-name that should be extracted from the loaded tsv-file. Then, your method should load the tsv-file using pandas and extract the correct column. Also, it should filter the resulting values with the attribute `subject_names` such that you only extract y-values for the subjects that are actually included in X (using the `loc` method). The resulting (filtered) y-values should be stored *as a numpy array* in a *new* attribute called `y`. So, after calling `add_y`, mvp should contain the attribute `y`.<br><br>\n",
    "\n",
    "So, calling the method would look something like this:<br> `my_mvp.load_y(path_to_tsv='/some/path/to/tsv-file.tsv', variable='Raven_raw')`<br>\n",
    "Then, after calling `load_y`, the attribute `y` should be accessible (i.e. mvp.y should exist).<br><br>\n",
    "\n",
    "3. Add another method, named `median_split_y` that takes no arguments (apart from *self*) and transforms the internal `y`-attribute into only 0 and 1 values using a median split.<br><br>\n",
    "\n",
    "In the code-cell below your copy-pasted code for your Mvp class, we included some code to check whether your extension of Mvp is correct. If this cell runs without errors, you completed the assignment correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mvp():\n",
    "    \"\"\" Custom class to load, organize, and process multivoxel MRI patterns. \"\"\"\n",
    "    \n",
    "    def __init__(self, paths):\n",
    "        \n",
    "        self.paths = paths\n",
    "        \n",
    "    def load(self, voxel_dims=(91, 109, 91)):\n",
    "\n",
    "        self.subject_names = [path.split('/')[4] for path in self.paths]\n",
    "        X = np.zeros((len(self.paths), np.prod(voxel_dims)))\n",
    "\n",
    "        # Start your loop here!\n",
    "        for i, path in enumerate(self.paths):\n",
    "    \n",
    "            X[i, :] = nib.load(path).get_data().ravel()\n",
    "        \n",
    "        self.X = X\n",
    "    \n",
    "    def standardize(self):\n",
    "        self.X = (self.X - self.X.mean(axis=0)) / self.X.std(axis=0)\n",
    "        \n",
    "    def apply_mask(self, path_to_mask, threshold):\n",
    "        \n",
    "        mask = nib.load(path_to_mask).get_data()\n",
    "        mask_bool = mask > threshold\n",
    "        self.X = self.X[:, mask_bool.ravel()]\n",
    "        \n",
    "    def load_y(self, path, variable):\n",
    "        \n",
    "        df = pd.read_csv(path, sep='\\t', index_col=0)\n",
    "        y = df[variable]\n",
    "        y = y.loc[self.subject_names]\n",
    "        self.y = np.array(y)\n",
    "        \n",
    "    def median_split_y(self):\n",
    "        \n",
    "        self.y = (mvp.y > np.median(self.y)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You extended the load() method correctly!\n",
      "You implemented the load_y() method correctly!\n",
      "You implemented the median_split_y() method correctly!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to check your Mvp extension\n",
    "# If there are no errors, you've done it correctly\n",
    "\n",
    "mvp = Mvp(subset_paths)\n",
    "mvp.load()\n",
    "\n",
    "# Check if X-attribute exists\n",
    "assert(hasattr(mvp, 'X'))\n",
    "\n",
    "# Check if subject_names attribute exists\n",
    "assert(hasattr(mvp, 'subject_names'))\n",
    "\n",
    "# Check if the length of subject_names is the same as the rows in X\n",
    "assert(len(mvp.subject_names) == mvp.X.shape[0])\n",
    "\n",
    "print('You extended the load() method correctly!')\n",
    "\n",
    "# Now, call the add_y method\n",
    "mvp.load_y(path_to_tsv, 'Raven_raw')\n",
    "\n",
    "# Check if y-attribute exists\n",
    "assert(hasattr(mvp, 'y'))\n",
    "\n",
    "# Check if y is indeed a numpy array\n",
    "assert(isinstance(mvp.y, np.ndarray))\n",
    "\n",
    "# Check if y is correctly filtered (i.e. only the subjects that are in subject_names)\n",
    "assert(len(mvp.y) == len(mvp.subject_names) == mvp.X.shape[0])\n",
    "\n",
    "print('You implemented the load_y() method correctly!')\n",
    "\n",
    "# Call the median_split_y() method\n",
    "mvp.median_split_y()\n",
    "\n",
    "# Check if y is now binary {0, 1}\n",
    "assert(all(np.unique(mvp.y) == [0, 1]))\n",
    "\n",
    "# Check if nothing weird happened to the length of y\n",
    "assert(len(mvp.y) == mvp.X.shape[0])\n",
    "\n",
    "print('You implemented the median_split_y() method correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "Awesome, we now have everything we need (X and y) packaged neatly in an instance of your Mvp-class! As might have become clear by now, using classes to \"package\"/organize code can be quite efficient, especially if you're modifying variables a lot (like preprocessing of X). \n",
    "\n",
    "In the rest of the tutorial, you'll finally learn how to actually implement decoding pipelines. As discussed in the lecture, typical pipelines consist of the following elements:\n",
    "\n",
    "* Data partitioning\n",
    "* Feature selection/extraction\n",
    "* Model fitting (on train-set)\n",
    "* Model cross-validation (on test-set)\n",
    "* Calculate model performance\n",
    "* Statistical analyses of performance\n",
    "* Optional: feature visualization\n",
    "\n",
    "In the rest of the tutorial, we'll discuss these topics in a slightly different order. First, we'll discuss how you can fit and cross-validate scikit-learn models, and while we're at it, how to calculate model performance (here: accuracy). Subsequently, you'll learn how to embed these concepts in fully cross-validated K-fold data partitioning schemes. Then, we'll extend our pipelines with feature selection methods. Finally, we'll show you how to implement a permutation test and briefly show you how to visualize features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model fitting & cross-validation\n",
    "First, we'll show you how to use scikit-learn models. In fact, the most useful functionality of scikit-learn is probably that they made fitting and cross-validating models (or, in scikit-learn lingo: estimators) trivially easy. \n",
    "\n",
    "Note that in the upcoming example, we are going to fit the model on *all* our samples - this is something you'd never do of course (you always need to cross-validate it to a new, independent sample). \n",
    "\n",
    "Anyway, let's import a scikit-learn model: the (linear) support vector classifier ([SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), which is one of the most-often used models in fMRI pattern analyses. In scikit-learn, this model is part of the (suprising...) `svm` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn is always imported as 'sklearn'\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most of scikit-learn's functionality, SVC is a *class*. So, let's initialize an SVC-object! One important argument that this object needs is the [\"kernel\"](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html) you want the model to have. Basically, the kernel determines how to treat/process your features: linearly, or non-linearly (such as the `kernel='rbf'` or `kernel='poly'` options). As discussed in the lecture, most often a linear kernel is the best option (as non-linear kernels seem to overfit very quicly). \n",
    "\n",
    "To initialize an SVC-object with a linear kernel, just do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = CLassiFier\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now we have initialized an SVC-object with a linear kernel. Now, you need to do two thing to get the prediction for each sample (i.e. whether they're predicted as 0 or 1): fit, using the method `fit(X, y)`, and predict the class (i.e. 0 or 1) for each class using the method `predict(X)`. Basically, in the `fit` method, the parameters of the model (i.e. $\\beta$) are calculated. Then, in the `predict` method, the parameters are used to generate a prediction for each sample (i.e. 0 or 1). \n",
    "\n",
    "Let's first look at the `fit` method. As you can see, the `fit(X, y)` method with two parameters: X (a samples-by-features matrix) and y (a vector of length n-samples). Let's do that for our data stored in our mvp-object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted SVC ...\n"
     ]
    }
   ],
   "source": [
    "clf.fit(mvp.X, mvp.y)\n",
    "print('Fitted SVC ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling the fit() method, the clf-object contains an attribute `coef_` that represent the model's parameters ('coefficients' in scikit-learn lingo, i.e. $\\beta$). Let's check that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of coefficients: (1, 902629)\n"
     ]
    }
   ],
   "source": [
    "coefs = clf.coef_\n",
    "print(\"Shape of coefficients: %r\" % (coefs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, just like we expected: the `coef_` attribute is exactly the same size as the number of voxels in our X-matrix (i.e. 91\\*109\\*91). \n",
    "\n",
    "Anyway, usually, you don't do anything directly with the weights (perhaps only if you want to do anything with feature visualization): scikit-learn handles that for you. What you *do* want, is an actual prediction ($\\hat{y}$) for our samples!\n",
    "\n",
    "To get this, simply call the `predict(X)` method of the model, which returns the predictions as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions for my samples are: array([0, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf.predict(mvp.X)\n",
    "print(\"The predictions for my samples are: %r\" % y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical next step is to assess how good the model was in predicting the class of the samples. A straightforward metric to summarize performance is *accuracy* which can be defined as: \n",
    "\n",
    "\\begin{align}\n",
    "accuracy = \\frac{number\\ of\\ correct\\ predictions}{number\\ of\\ predictions}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Can you calculate the accuracy of the above model? Hint 1: you need to compare the true labels (i.e. mvp.y) with the predicted labels (i.e. y_hat). Hint 2: if you do arithmetic with boolean values (i.e. `True` and `False`), `True` is interpreted as 1 and `False` is interpreted as 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-269-c79b040c0188>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-269-c79b040c0188>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    accuracy =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implement your to-do here!\n",
    "accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done the ToDo correctly above, you should have found out that the accuracy was 1.0 - a perfect score! \"Awesome! Nature Neuroscience material!\", you might think. But, as is almost always the case: if it's too good to be true, it probably *is* indeed too good to be true.\n",
    "\n",
    "So, what is the issue here? Well, we didn't cross-validate the model! We fitted it on all the samples in the mvp-object and predicted the *same* samples, which leads to optimistic estimate of model performance. Such optimistic estimates in uncross-validated models are especially likely when there are many more features (here: voxels) than samples (here: subjects). In other words, we are probably *overfitting* the model here.\n",
    "\n",
    "Thus, let's check what happens if we actually cross-validate the model. To do so, we need to partition the data into a train- and test-set. For this example, we'll use a simple hold-out scheme, in which we'll reserve half of the data for the test-set (we'll discuss more intricate cross-validation schemes such as K-fold in the next section).\n",
    "\n",
    "Below, we index `X` and `y` such that the train-set will contain all odd-numbered samples and the test-set will contain all even-number samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (5, 902629)\n",
      "Shape y_train: (5,)\n",
      "Shape X_test: (5, 902629)\n",
      "Shape y_test: (5,)\n"
     ]
    }
   ],
   "source": [
    "X_train = mvp.X[1::2]\n",
    "print(\"Shape X_train: %r\" % (X_train.shape,))\n",
    "\n",
    "y_train = mvp.y[1::2]\n",
    "print(\"Shape y_train: %r\" % (y_train.shape,))\n",
    "\n",
    "X_test = mvp.X[0::2]\n",
    "print(\"Shape X_test: %r\" % (X_test.shape,))\n",
    "\n",
    "y_test = mvp.y[0::2]\n",
    "print(\"Shape y_test: %r\" % (y_test.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: now it's up to you to actually implement the *cross-validated* equivalent of the fit/predict procedure we showed you before. So, fit your model on `X_train` and `y_train` and then predict `X_test`. Calculate the cross-validated accuracy by comparing the predictions with `y_test`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! You actually performed your first proper decoding analysis in the ToDo above! There are however a couple of things we can do to improve the efficiency and results. One thing we can do is to use the data more efficiently in cross-validation. In the previous example, we split the data into two sets and fit the model on one (the train-set) and cross-validated to the other (the test-set). But as you've heard in the lecture, there is actually a way to \"re-used\" the data by using K-fold cross-validation, in which data is iteratively partitioned in train- and test-sets. This is explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data partitioning: K-fold cross-validation\n",
    "As discussed in the lecture, there are two principled ways of cross-validation: \n",
    "\n",
    "* Hold-out cross-validation;\n",
    "* K-fold cross-validation;\n",
    "\n",
    "In the previous section we implemented a form of hold-out cross-validation, which can be seen as a kind of \"one-shot cross-validation\". In fMRI data-sets, however, we usually have few samples (simply because MRI-data is expensive to acquire!), so K-fold cross-validation is more common than hold-out cross-validation in fMRI pattern analyses because it allows you to reuse the data.\n",
    "\n",
    "Now, we can finally use some of scikit-learn's functionality. We are going to use the [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) object from scikit-learn's model_selection module. Click the highlighted link above and read through the manual to see how it works.\n",
    "\n",
    "Importantly, if you're dealing with a classification analysis, always use *Stratified*KFold (instead of the regular [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)), because this version makes sure that each fold contains the same proportion of the different classes (here: 0 and 1). Anyway, enough talking. Let's initialize a StratifiedKFold object with 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scikit-learn is imported as 'sklearn'\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# They call folds 'splits' in scikit-learn\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have a StratifiedKFold object now, but not yet any indices for our folds (i.e. indices to split X and y into different samples). To do that, we need to call the `split(X, y)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = skf.split(mvp.X, mvp.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we created the variable `folds` which is officially a [generator](https://wiki.python.org/moin/Generators) object, but just think of it as a type of list (with indices) which is specialized for looping over it. Each entry in `folds` is a tuple with two elements: an array with train-indices and an array with test-indices. Let's demonstrate that\\*:\n",
    "\n",
    "-------------\n",
    "\\* Note that you can only run the cell below once. After running it, the `folds` generator object is \"exhausted\", and you'll need to call `skf.split(mvp.X, mvp.y)` again in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Train-indices: [1 3 4 5 6 7 8 9]\n",
      "Test-indices: [0 2]\n",
      "\n",
      "Processing fold 2\n",
      "Train-indices: [0 2 4 5 6 7 8 9]\n",
      "Test-indices: [1 3]\n",
      "\n",
      "Processing fold 3\n",
      "Train-indices: [0 1 2 3 6 7 8 9]\n",
      "Test-indices: [4 5]\n",
      "\n",
      "Processing fold 4\n",
      "Train-indices: [0 1 2 3 4 5 8 9]\n",
      "Test-indices: [6 7]\n",
      "\n",
      "Processing fold 5\n",
      "Train-indices: [0 1 2 3 4 5 6 7]\n",
      "Test-indices: [8 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for fold in folds:\n",
    "    \n",
    "    print(\"Processing fold %i\" % i)\n",
    "    # Here, we unpack fold (a tuple) to get the train- and test-indices\n",
    "    train_idx = fold[0]\n",
    "    test_idx = fold[1]\n",
    "    \n",
    "    print(\"Train-indices: %s\" % train_idx)\n",
    "    print(\"Test-indices: %s\\n\" % test_idx)\n",
    "    \n",
    "    i += 1\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, StratifiedKFold determined that for the first fold, sample 1 to 9 should be used for training and sample 0 and 2 (remember, Python uses 0-based indexing!) should be used for testing.\n",
    "\n",
    "Now, we know how to access the train- and test-indices, but we haven't *actually* indexed our X and y (i.e. mvp.X and mvp.y) in the for-loop over folds. Nor have we actually fit the model on the train-set and cross-validated this to the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: in the code-cell below, complete the statements by indexing mvp.X and mvp.y to create four different objects in every fold: X_train, y_train, X_test, y_test. Also, we created a new classifier-object (clf) for you based on a different model: scikit-learn's `LogisticRegression` to show you that *every* model in scikit-learn works the same. Use this classifier to fit on the train-set and predict the test-set in every fold. Then, calculate the (cross-validated) accuracy in every fold. Keep track of the accuracies across folds, and after the loop over folds, calculate the average accuracy across folds.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-275-94b4e270e3b4>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-275-94b4e270e3b4>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    X_train =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf now is a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "# run split() again to generate folds\n",
    "folds = skf.split(mvp.X, mvp.y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    # This is a slightly more concise way to unpack tuples\n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # Complete these statements by indexing mvp.X and mvp.y with train_idx and test_idx\n",
    "    X_train = \n",
    "    y_train = \n",
    "    \n",
    "    X_test = \n",
    "    y_test = \n",
    "    \n",
    "    # ToDo: call fit (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-fold cross-validation instead of hold-out cross-validation allowed us to make our analysis a little more efficient (by reusing samples). Another method that (usually) improves performance in decoding analyses is feature selection/extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection/extraction\n",
    "As discussed before, a small sample/feature-ratio often results in overfitting (optimistic performance estimates on train-set). It follows then that reducing the amount of features often has a beneficial effect on cross-validated performance estimates.\n",
    "\n",
    "Feature reduction can be achieved in two principled ways:\n",
    "\n",
    "* feature extraction: transform your features into a set of lower-dimensional components;\n",
    "* feature selection: select a subset of features\n",
    "\n",
    "Examples of feature extraction are PCA (i.e. transform voxels to orthogonal components) and averaging features within brain regions from an atlas. Because feature extraction often does not use any information from the labels (`y`), this step does not need to be cross-validated (but you *can* cross-validate, e.g., PCA by estimating the components on the train-set only). \n",
    "\n",
    "Examples of feature selection are ROI-analysis (i.e. restricting your patterns to a specific ROI in the brain) and \"univariate feature selection\" (UFS). This latter method is an often-used data-driven method to select features based upon their univariate difference. \n",
    "\n",
    "Fortunately, scikit-learn has a bunch of feature selection/extraction objects for us to use. These objects (\"transformers\" in scikit-learn lingo) work similarly to estimators: they also have a `fit(X, y)` method, in which for example the univariate differences (in UFS) or PCA-components (in PCA-driven feature extraction) are computed. Then, instead of having a `predict(X)` method, transformers have a `tranform(X)` method.\n",
    "\n",
    "Let's take a closer look at how UFS can be implemented using scikit-learn. Basically, it follows the following structure:\n",
    "\n",
    "`transformer = Selector(score_func=ufs_function, other_args)`\n",
    "\n",
    "Here, the `score_func` parameter takes a function that implements a univariate statistical test (like an [f-test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) or a [chi-square test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)). The `Selector` class determines how to select the subset of features based on the result of the `score_func`. \n",
    "\n",
    "For example, the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) selector selects the best `K` features based on their scores from the statistical test. For example, the following transformer would select the best 100 features based on their F-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# f_classif is a scikit-learn specific implementation of the F-test\n",
    "select100best = SelectKBest(score_func=f_classif, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a selector is `SelectFwe` which selects only features with statistics-values corresponding to p-values lower than a set alpha-level. For example, the following transformer would only select features with p-values from a chi-square test lower than 0.01: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "selectfwe_transformer = SelectFwe(score_func=chi2, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: The functions `f_classif` and `chi2` are functions to calculate statistics for when the the DV is categorical (i.e. in classification scenarios). Can you think of a function that does the same when your DV is continuous (i.e. in regression scenarios)? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does this work in practice? We'll show you an (not cross-validated!) example using the select100best transformer initialized earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902629\n"
     ]
    }
   ],
   "source": [
    "# Fit the transformer ...\n",
    "select100best.fit(mvp.X, mvp.y)\n",
    "\n",
    "# ... which calculates the following attributes (.scores_ and .pvalues_)\n",
    "# Let's check them out\n",
    "scores, pvalues = select100best.scores_, select100best.pvalues_\n",
    "\n",
    "# As you can see, each voxel gets its own score (in this case: an F-score)\n",
    "print(scores.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize these scores in brain space (it's a coronal slice sideways ...): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+UMYlP8HUtzZ+CzXS5IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f492b870290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scores_3d = scores.reshape((91, 109, 91))\n",
    "plt.imshow(scores_3d[:, 75, :], origin='lower', cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after fitting the transformer, we can call the `transform(X)` method that will actually select the subset according to the selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mvp.X before transform: (10, 902629) ...\n",
      "... and shape of mvp.X after transform: (10, 100).\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of mvp.X before transform: %r ...\" % (mvp.X.shape,))\n",
    "X_after_ufs = select100best.transform(mvp.X)\n",
    "\n",
    "print(\"... and shape of mvp.X after transform: %r.\" % (X_after_ufs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the transformer correctly selected a subset of 100 voxels from our mvp.X matrix! Now, both selectors were fit on the entire dataset, which is often course not how it should be done: because they use information from the labels (`y`), this step should be cross-validated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 2** (2 points): \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "folds = skf.split(mvp.X, mvp.y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "select50best = SelectKBest(score_func=f_classif, k=50)\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # ToDo: make X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # ToDo: call the select50best fit method (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Permutation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
