{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2: decoding analyses\n",
    "\n",
    "This week's tutorial is about how to implement decoding analyses in Python. We'll be looking at the implementation of concepts like K-fold cross-validation, feature-selection/extraction, model fitting/prediction, permutation testing, and feature visualization. To do so, we'll use the [`scikit-learn`](http://scikit-learn.org) machine learning package in Python (the de-facto and most-used ML package in Python). In this tutorial, we'll use the PIOP-dataset (**P**opulation **I**maging **O**f **P**sychology), a large-scale (>240 subjects) multimodal MRI-dataset, including structural (gray-matter [VBM](https://en.wikipedia.org/wiki/Voxel-based_morphometry) and white-matter [TBSS](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS)), resting-state (incl. [dual regression](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/DualRegression) data), and task-based functional data. Along with this set of MRI-scans, the PIOP-study also acquired a set of behavioral (psychometric) variables from various questionnaires and behavioral task (outside the scanner) of the same participants who were scanned. As such, this makes it an excellent data-set for decoding analyses in which you can use neural patterns (from any MRI modality; `X`) to predict a behavioral variable (`y`). This is, by the way, also one of the data-sets that you can use for your final project (more info on Blackboard/week_4).\n",
    "\n",
    "For this tutorial, we'll focus on a specific decoding analysis in which *we try to predict intelligence (y) from brain patterns (X) associated with working memory processing* (based on the same data as you practiced with in the tutorial of week 1). Throughout the tutorial, we'll discuss all the steps necessary to answer this research question. (Mini-ToThink: is this a within- or between-subject analysis?)\n",
    "\n",
    "In terms of skills, after this tutorial you are be able to:\n",
    "\n",
    "* Load and appropriately transform dependent variables (`y`) corresponding to your patterns (`X`);\n",
    "* Apply basic [preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) to your patterns;\n",
    "* Implement [feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) and extraction processes;\n",
    "* Build fully cross-validated [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) using scikit-learn;\n",
    "* Run appropriate statistics (including permutation tests) on your model performance scores;\n",
    "\n",
    "Notably, we do *not* cover feature visualization because it is quite complicated to implement, especially in combination with feature-selection. Also, \"mapping\" should not be your goal in decoding studies, anyway. \n",
    "\n",
    "That said, in combination with the knowledge/skills you acquired from last week's tutorial, you'll be able to implement a complete decoding pipeline yourself at the end of this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data representation (yet again)\n",
    "Before we'll start with the machine learning stuff, we have to load in our patterns again. Like last week, you are going to use a custom `Mvp` class for this again, which we'll extend throughout this tutorial. You may use your own class from last week (given that it contains correct implementations of the load, standardize and apply_mask methods), or use the one in the solutions-notebook from last week. In any case, copy the code for the Mvp-class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Copy here below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given our research question, we're going to use the (between-subject!\\*) patterns of the working-memory task again. More specifically, we are going to use the patterns from the \"active-passive\" contrast of each subject (just like last week).\n",
    "\n",
    "---------------\n",
    "\\* It's a between-subject analysis because intelligence is a factor that varies at the subject level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'd like to do now is to load in the tstat3 patterns from all subjects who completed the working-memory task in the scanner. We put this dataset in the `/home/Public/FirstLevel_piop` folder. This folder (and all its subdirectories) is accessible (read-only, so you cannot modify anything) for all NIPA-students. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Click on the \"Files\" icon on your desktop and navigate (just by clicking, not in the terminal) to this directory (note: you have to go up one directory relative to your `home` directory).<br><br>\n",
    "\n",
    "Check out the structure. You should see a bunch of subdirectories with subject-names (e.g. pi0010, pi0204, pi0084). These directories contain the first-level data (.feat directories) from the different task-based MRI-runs and some separate nifti-files (we'll get to these later). As you can see, we already estimated the (between-subject) patterns for you (which can be found in the .feat directories)! Go to a random subject's directory and click on the `*piopwm.feat` directory (note that not *all* subjects have this directory, because some subjects did not complete this run in the scanner due to technical issues or time issues). Check out the `design.png` file and the `reg_standard` directory (which files are in there? Do you understand what each file represents?). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is structured in an hierarchical way:\n",
    "\n",
    "`/home/Public/FirstLevel_piop/{subject-no}/{task}.feat/reg_standard/{tstat-files}.nii.gz`\n",
    "\n",
    "Given this structure, it's easy to `glob` a set of tstat-files for a specific task across all subjects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: use glob to find all the paths to the tstat3-files of the piopwm task. (Hint 1, you will need *two* wildcards in your search-string; Hint 2, you should find 217 paths in total)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement the todo here (insert the right search-query in glob()):\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the paths, such that the paths are sorted by subject-number (pi0010 to pi0248). Also, let's not use *all* data (all subjects) at once, because that's actually quite memory-intensive and we don't want the TUX-server to crash. Therefore, we'll select only every third path (i.e. path 1, 4, 7, 10, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = sorted(paths)\n",
    "print(\"Number of paths was: %i\" % len(paths))\n",
    "subset_paths = paths[1::3]\n",
    "print(\"Number of paths is now: %i\" % len(subset_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have 72 paths (sorted by subject-number) to the subjects' tstat3 files. Let's initialize the Mvp object and call the `load()` method! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: before you initialize the Mvp object and load the patterns, can you predict what shape the `X`-attribute of the Mvp-object will be? (Hint: images in MNI152 2mm space are of shape 91\\*109\\*91)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, less thinking, more doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mvp = Mvp(paths=subset_paths)\n",
    "mvp.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the `X`-attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape of X is: %s\" % (mvp.X.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about standardization\n",
    "We briefly touched upon the issue of \"standardization\" last week, but the process of standardization is especially important in decoding analyses, so it warrants some extra explanation. \n",
    "\n",
    "As explained before, standardization refers to the process of converting numbers in a vector to the (signed) number of standard deviations it deviates from the mean of the vector. Formally, for a vector `x` with mean $\\bar{x}$ and standard deviation $s_{x}$ and length `N`, the standardized value of element $x_{i}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "x_{i} = \\frac{x_{i} - \\bar{x}}{s_{x}}\n",
    "\\end{align}\n",
    "\n",
    "Practically, standardization in machine learning is applied to each column (i.e. feature), ensuring that each feature (in decoding: voxel) is on the same scale. In other words, standardization ensures that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "\"But *why* do you have to standardize your features?\", you might reasonably ask. Well, that has to do with the way many ML models calculate their parameters (i.e. $\\beta$; this process is called 'gradient descent'). For most models, this process may fail to converge and thus is unable to calculate the model's parameters is not all features are on the same scale. \n",
    "\n",
    "Anyway, just remember that *before you do any model fitting* your features should be standardized\\*. Fortunately, you already implemented this yourself last week as a method in your mvp-class: the `standardize()` method! So, before we go on, let's just standardize our features:\n",
    "\n",
    "-----------\n",
    "\\* There are also other ways to get your features on the same scale, like min-max scaling, but standardization is by far the most often used method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mvp.standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we got the (standardized) patterns (`X`) that we wanted. But as you might have noticed, we're still missing one crucial component for our decoding analysis: `y` (our to-be-decoded feature)! For our research question, we chose to focus on decoding intelligence, but how do we load and represent this in our pipeline? This is what the next section is about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding a dependent variable (`y`)\n",
    "While there is kind of a generic way to load in voxel patterns, there is usually not a single way to load in your dependent variable (y), because the exact factor that represents `y` dependent on your exact research question (and also depends how you have stored this data on disk). \n",
    "\n",
    "We'll focus on extracting a DV from a common tsv-file (like a csv-file, but not with comma- but tab-delimited values). Before we do that, just open the file and take a look at its structure. Just go to your desktop, double-click \"Files\", go up one directory (to `/home`) and go to `/home/Public/FirstLevel_piop`. Scroll aaaall the way down (past all the pi\\* folders) and you'll find a file called `PIOP_BEHAV_NIPA.tsv` file. Double-click this, and press \"Display\". A \"Text Import\" window will pop up - select here \"Tab\" under the \"Separated by\" option. Then, click \"OK\". \n",
    "\n",
    "You'll see a spreadsheet with the subject-names in the first column and different variables in the columns. Basically, you can choose any variable in the columns as your to-be-decoded factor! For our example, we will pick the 'Raven' variable, which represents the subjects' (continuous) scores on the [Raven](https://en.wikipedia.org/wiki/Raven%27s_Progressive_Matrices) test, which aims to measure fluid intelligence. \n",
    "\n",
    "### 2.1. Loading behavioral data\n",
    "To load in the tsv-file, we'll use `pandas`, a Python package for working with tabular data. It's main data structure is called the `DataFrame`, which is very similar to the dataframe-object in R. (Working with pandas is beyond the scope of this course, but you'll see it now and then throughout the course when we work with tsv-files.)\n",
    "\n",
    "We'll walk you through how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # This is how pandas usually is imported\n",
    "\n",
    "path_to_tsv = '/home/Public/FirstLevel_piop/PIOP_BEHAV_NIPA.tsv'\n",
    "\n",
    "# the read_csv function is the standard way to load in spreadsheet-like data\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', index_col=0)\n",
    "\n",
    "# Let's see how this looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the DataFrame is simply a spreadsheet-like representation of the data, with the subjects represented as different rows and the different behavioral/demographic variables represented as columns. Now, we'd like to extract the `Raven_raw` columns, which represents the raw (summed) scores on the Raven-test for each subject. To do so, we can simply index the `df` variable with the string \"Raven_raw\" (just like indexing a dictionary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['Raven_raw']\n",
    "print(\"Length of y is: %i\\n\" % y.shape[0])\n",
    "print(\"y looks like:\\n\\n%r\" % y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is now a subset of the original dataframe (df) that only contains the Raven scores and a corresponding \"index\", which refers to the subject it belongs to. \n",
    "\n",
    "But we have a problem! \n",
    "\n",
    "`y` now contains 248 values (from 248 subjects), but our X attribute contains patterns from only 70 subjects (the subset selected earlier). For our analysis, the samples in our patterns (X) should match the entries in our dependent variable (y). How do we do this? Well, we need to extract which subjects were included in X and then use those subjects to \"index\" y. \n",
    "\n",
    "Below, we show you a way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we need to extract which subjects were actually included in X.\n",
    "# We do this by extracting this info from the \"subset_paths\" from earlier\n",
    "# using a list-comprehension (a fancy for loop)\n",
    "subject_names = [path.split('/')[6] for path in subset_paths]\n",
    "\n",
    "print(subject_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we use the pandas dataframe \"loc\" method, which slices the DataFrame by their\n",
    "# index names (and we subsequently transform the result to a numpy array)\n",
    "y_new = np.array(y.loc[subject_names])\n",
    "\n",
    "# And we can see now that the length of y is the same as the number of samples in X\n",
    "print(mvp.X.shape)\n",
    "print(y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a set of patterns (X) with a corresponding vector representing the to-be-decoded feature (y). There is, however, still one issue worth discussing. At this moment, `y` is a continuous measure (sum-score of performance on the Raven test). It follows then that our decoding analysis would use regression, but we will actually recode `y` such that it is categorical, as explained in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Continuous vs. categorical DVs\n",
    "In some scenario's you might want to recode your continuous dependent variable (y) into a categorical one. One reason is simply because fMRI patterns are incredibly noisy, and classification analyses are more robust than regression analyses. Note, though, that if you recode your continuous DV into a categorical one, your conclusions about your model also become weaker: regression models allow you to explain/model variance across a larger interval than classification models.\n",
    "\n",
    "For example, would you be more impressed if told you I could model predict your IQ with an error of +/- 2 points (a continuous DV) or if I told you I could only predict whether your IQ is higher or lower than 100 (a categorical DV)? Given that your dependent variable is truly continuous, it's better to subject it to a regression analysis in terms of strength of your conclusion, but given the very noisy nature of fMRI data it may be advantageous to trade in some of this inferential strength for some extra SNR by recoding your DV into a categorical variable. \n",
    "\n",
    "This is probably also the reason that *classification* analyses are way more popular in the fMRI community than *regression* analyses. That's why we'll recode the Raven score into a categorical variable and continue with that for the rest of the tutorial. \n",
    "\n",
    "But how should we recode our DV? There are various ways to do this, but we'll use the median-split: any values above the median of `y` will be coded as 1 and any values below the median of `y` will be coded 0. Thus, our updated decoding goal is to predict whether someone has a high intelligence (`y = 1`) or low intelligence (`y = 0`).  \n",
    "\n",
    "We'll show you how to implement this median-split transfor below ... No wait, you know what, by now you guys should be quite the Python experts: try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Transform mvp.y into categorical values {0, 1} using a median-split (you can use the numpy-function `median` to do this, i.e. np.median(some_array)). Hint: maybe you can use the [astype()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html) method of numpy arrays to convert boolean arrays to integer arrays?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform mvp.y into zeros and ones using a median split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, finally, we have everything we need for a decoding analysis: a sample-by-voxel array (X) representing the patterns and a vector with the corresponding to-be-decoded (categorical) variable (y). This is really all we need for a decoding analysis, we promise.\n",
    "\n",
    "But the process of loading in a y-variable, making sure it corresponds to the same subjects contained in X, and transforming it using a median-split is kinda messy. For the following interim assignment, you have to incorporate this process in your Mvp class! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 1** (6 points): In this assigment, you will extend your Mvp class such that it is able to load in a y-variable from a tsv-file and index it such that it contains the data from the same subjects that were included in the X-attribute. So, first copy-paste the code for your class in the code-cell below. Then, do three things:<br><br>\n",
    "\n",
    "1. *Within* your load() method, extract the subject-names from the `paths` attribute and store the results (which should be a list with subject-names, like this: ['pi0010', 'pi0011', 'pi0012', ..., etc.]) in a *new* attribute named 'subject_names'. You can use the list-comprehension defined earlier if you want to extract the subject-names. So, after calling the load() method, your mvp-object should also contain the attribute `subject_names` which represents which subjects the X-attribute is based upon.<br><br>\n",
    "\n",
    "2. Then, add a new method named `load_y` that takes two arguments: a path (a string) to a tsv-file and a variable name (a string) referring to the column-name that should be extracted from the loaded tsv-file. Then, your method should load the tsv-file using pandas and extract the correct column. Also, it should filter the resulting values with the attribute `subject_names` such that you only extract y-values for the subjects that are actually included in X (using the `loc` method). The resulting (filtered) y-values should be stored *as a numpy array* in a *new* attribute called `y`. So, after calling `add_y`, mvp should contain the attribute `y`.<br><br>\n",
    "\n",
    "So, calling the method would look something like this:<br> `my_mvp.load_y(path_to_tsv='/some/path/to/tsv-file.tsv', variable='Raven_raw')`<br>\n",
    "Then, after calling `load_y`, the attribute `y` should be accessible (i.e. mvp.y should exist).<br><br>\n",
    "\n",
    "3. Add another method, named `median_split_y` that takes no arguments (apart from *self*) and transforms the internal `y`-attribute into only 0 and 1 values using a median split.<br><br>\n",
    "\n",
    "In the code-cell below your copy-pasted code for your Mvp class, we included some code to check whether your extension of Mvp is correct. If this cell runs without errors, you completed the assignment correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extend the class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to check your Mvp extension\n",
    "# If there are no errors, you've done it correctly\n",
    "\n",
    "mvp = Mvp(subset_paths)\n",
    "mvp.load()\n",
    "\n",
    "# Check if X-attribute exists\n",
    "assert(hasattr(mvp, 'X'))\n",
    "\n",
    "# Check if subject_names attribute exists\n",
    "assert(hasattr(mvp, 'subject_names'))\n",
    "\n",
    "# Check if the length of subject_names is the same as the rows in X\n",
    "assert(len(mvp.subject_names) == mvp.X.shape[0])\n",
    "\n",
    "print('You extended the load() method correctly!')\n",
    "\n",
    "# Now, call the add_y method\n",
    "mvp.load_y(path_to_tsv, 'Raven_raw')\n",
    "\n",
    "# Check if y-attribute exists\n",
    "assert(hasattr(mvp, 'y'))\n",
    "\n",
    "# Check if y is indeed a numpy array\n",
    "assert(isinstance(mvp.y, np.ndarray))\n",
    "\n",
    "# Check if y is correctly filtered (i.e. only the subjects that are in subject_names)\n",
    "assert(len(mvp.y) == len(mvp.subject_names) == mvp.X.shape[0])\n",
    "\n",
    "print('You implemented the load_y() method correctly!')\n",
    "\n",
    "# Call the median_split_y() method\n",
    "mvp.median_split_y()\n",
    "\n",
    "# Check if y is now binary {0, 1}\n",
    "assert(all(np.unique(mvp.y) == [0, 1]))\n",
    "\n",
    "# Check if nothing weird happened to the length of y\n",
    "assert(len(mvp.y) == mvp.X.shape[0])\n",
    "\n",
    "print('You implemented the median_split_y() method correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "Awesome, we now have everything we need (X and y) packaged neatly in an instance of your Mvp-class! As might have become clear by now, using classes to \"package\"/organize code can be quite efficient, especially if you're modifying variables a lot (like preprocessing of X). \n",
    "\n",
    "In the rest of the tutorial, you'll finally learn how to actually implement decoding pipelines. As discussed in the lecture, typical pipelines consist of the following elements:\n",
    "\n",
    "* Data partitioning\n",
    "* Feature selection/extraction\n",
    "* Model fitting (on train-set)\n",
    "* Model cross-validation (on test-set)\n",
    "* Calculate model performance\n",
    "* Statistical analyses of performance\n",
    "* Optional: feature visualization\n",
    "\n",
    "In the rest of the tutorial, we'll discuss these topics in a slightly different order. First, we'll discuss how you can fit and cross-validate scikit-learn models, and while we're at it, how to calculate model performance (here: accuracy). Subsequently, you'll learn how to embed these concepts in fully cross-validated K-fold data partitioning schemes. Then, we'll extend our pipelines with feature selection methods. Finally, we'll show you how to implement a permutation test and briefly show you how to visualize features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model fitting & cross-validation\n",
    "First, we'll show you how to use scikit-learn models. In fact, the most useful functionality of scikit-learn is probably that they made fitting and cross-validating models (or, in scikit-learn lingo: estimators) trivially easy. \n",
    "\n",
    "Note that in the upcoming example, we are going to fit the model on *all* our samples - this is something you'd never do of course (you always need to cross-validate it to a new, independent sample). \n",
    "\n",
    "Anyway, let's import a scikit-learn model: the (linear) support vector classifier ([SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), which is one of the most-often used models in fMRI pattern analyses. In scikit-learn, this model is part of the (suprising...) `svm` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn is always imported as 'sklearn'\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most of scikit-learn's functionality, SVC is a *class*. So, let's initialize an SVC-object! One important argument that this object needs is the [\"kernel\"](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html) you want the model to have. Basically, the kernel determines how to treat/process your features: linearly, or non-linearly (such as the `kernel='rbf'` or `kernel='poly'` options). As discussed in the lecture, most often a linear kernel is the best option (as non-linear kernels seem to overfit very quicly). \n",
    "\n",
    "To initialize an SVC-object with a linear kernel, just do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = CLassiFier\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now we have initialized an SVC-object with a linear kernel. Now, you need to do two thing to get the prediction for each sample (i.e. whether they're predicted as 0 or 1): fit, using the method `fit(X, y)`, and predict the class (i.e. 0 or 1) for each class using the method `predict(X)`. Basically, in the `fit` method, the parameters of the model (i.e. $\\beta$) are calculated. Then, in the `predict` method, the parameters are used to generate a prediction for each sample (i.e. 0 or 1). \n",
    "\n",
    "Let's first look at the `fit` method. As you can see, the `fit(X, y)` method with two parameters: X (a samples-by-features matrix) and y (a vector of length n-samples). Let's do that for our data stored in our mvp-object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(mvp.X, mvp.y)\n",
    "print('Fitted SVC ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling the fit() method, the clf-object contains an attribute `coef_` that represent the model's parameters ('coefficients' in scikit-learn lingo, i.e. $\\beta$). Let's check that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefs = clf.coef_\n",
    "print(\"Shape of coefficients: %r\" % (coefs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, just like we expected: the `coef_` attribute is exactly the same size as the number of voxels in our X-matrix (i.e. 91\\*109\\*91). \n",
    "\n",
    "Anyway, usually, you don't do anything directly with the weights (perhaps only if you want to do anything with feature visualization): scikit-learn handles that for you. What you *do* want, is an actual prediction ($\\hat{y}$) for our samples!\n",
    "\n",
    "To get this, simply call the `predict(X)` method of the model, which returns the predictions as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = clf.predict(mvp.X)\n",
    "print(\"The predictions for my samples are:\\n %r\" % y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical next step is to assess how good the model was in predicting the class of the samples. A straightforward metric\\* to summarize performance is *accuracy* which can be defined as: \n",
    "\n",
    "\\begin{align}\n",
    "accuracy = \\frac{number\\ of\\ correct\\ predictions}{number\\ of\\ predictions}\n",
    "\\end{align}\n",
    "\n",
    "---------------\n",
    "\\* There are waaaay [more metrics to summarize model performance](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) for classification models, such as precision, recall, F1-score, and ROC-AUC. These metrics are more appropriate than accuracy when you have *unbalanced classes*, i.e. more samples in one class (e.g. negative images) than in another class (e.g. positive images). Usually, however, experimental designs in fMRI pattern analyses are (more of less) balanced, so often you can just use accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Can you calculate the accuracy of the above model? Hint 1: you need to compare the true labels (i.e. mvp.y) with the predicted labels (i.e. y_hat). Hint 2: if you do arithmetic with boolean values (i.e. `True` and `False`), `True` is interpreted as 1 and `False` is interpreted as 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement your to-do here!\n",
    "accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done the ToDo correctly above, you should have found out that the accuracy was 1.0 - a perfect score! \"Awesome! Nature Neuroscience material!\", you might think. But, as is almost always the case: if it's too good to be true, it probably *is* indeed too good to be true.\n",
    "\n",
    "So, what is the issue here? Well, we didn't cross-validate the model! We fitted it on all the samples in the mvp-object and predicted the *same* samples, which leads to optimistic estimate of model performance. Such optimistic estimates in uncross-validated models are especially likely when there are many more features (here: voxels) than samples (here: subjects). In other words, we are probably *overfitting* the model here.\n",
    "\n",
    "Thus, let's check what happens if we actually cross-validate the model. To do so, we need to partition the data into a train- and test-set. For this example, we'll use a simple hold-out scheme, in which we'll reserve half of the data for the test-set (we'll discuss more intricate cross-validation schemes such as K-fold in the next section).\n",
    "\n",
    "Below, we index `X` and `y` such that the train-set will contain all odd-numbered samples and the test-set will contain all even-number samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = mvp.X[1::2]\n",
    "print(\"Shape X_train: %r\" % (X_train.shape,))\n",
    "\n",
    "y_train = mvp.y[1::2]\n",
    "print(\"Shape y_train: %r\" % (y_train.shape,))\n",
    "\n",
    "X_test = mvp.X[0::2]\n",
    "print(\"Shape X_test: %r\" % (X_test.shape,))\n",
    "\n",
    "y_test = mvp.y[0::2]\n",
    "print(\"Shape y_test: %r\" % (y_test.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: now it's up to you to actually implement the *cross-validated* equivalent of the fit/predict procedure we showed you before. So, fit your model on `X_train` and `y_train` and then predict `X_test`. Calculate the cross-validated accuracy by comparing the predictions with `y_test`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement your ToDo here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! You actually performed your first proper decoding analysis in the ToDo above! There are however a couple of things we can do to improve the efficiency and results. One thing we can do is to use the data more efficiently in cross-validation. In the previous example, we split the data into two sets and fit the model on one (the train-set) and cross-validated to the other (the test-set). But as you've heard in the lecture, there is actually a way to \"re-used\" the data by using K-fold cross-validation, in which data is iteratively partitioned in train- and test-sets. This is explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data partitioning: K-fold cross-validation\n",
    "As discussed in the lecture, there are two principled ways of cross-validation: \n",
    "\n",
    "* Hold-out cross-validation;\n",
    "* K-fold cross-validation;\n",
    "\n",
    "In the previous section we implemented a form of hold-out cross-validation, which can be seen as a kind of \"one-shot cross-validation\". In fMRI data-sets, however, we usually have few samples (simply because MRI-data is expensive to acquire!), so K-fold cross-validation is more common than hold-out cross-validation in fMRI pattern analyses because it allows you to reuse the data.\n",
    "\n",
    "Now, we can finally use some of scikit-learn's functionality. We are going to use the [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) object from scikit-learn's model_selection module. Click the highlighted link above and read through the manual to see how it works.\n",
    "\n",
    "Importantly, if you're dealing with a classification analysis, always use *Stratified*KFold (instead of the regular [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)), because this version makes sure that each fold contains the same proportion of the different classes (here: 0 and 1). Anyway, enough talking. Let's initialize a StratifiedKFold object with 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scikit-learn is imported as 'sklearn'\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# They call folds 'splits' in scikit-learn\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have a StratifiedKFold object now, but not yet any indices for our folds (i.e. indices to split X and y into different samples). To do that, we need to call the `split(X, y)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = skf.split(mvp.X, mvp.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we created the variable `folds` which is officially a [generator](https://wiki.python.org/moin/Generators) object, but just think of it as a type of list (with indices) which is specialized for looping over it. Each entry in `folds` is a tuple with two elements: an array with train-indices and an array with test-indices. Let's demonstrate that\\*:\n",
    "\n",
    "-------------\n",
    "\\* Note that you can only run the cell below once. After running it, the `folds` generator object is \"exhausted\", and you'll need to call `skf.split(mvp.X, mvp.y)` again in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for fold in folds:\n",
    "    \n",
    "    print(\"Processing fold %i\" % i)\n",
    "    # Here, we unpack fold (a tuple) to get the train- and test-indices\n",
    "    train_idx = fold[0]\n",
    "    test_idx = fold[1]\n",
    "    \n",
    "    print(\"Train-indices: %s\" % train_idx)\n",
    "    print(\"Test-indices: %s\\n\" % test_idx)\n",
    "    \n",
    "    i += 1\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, StratifiedKFold determined that for the first fold, sample 1 to 9 should be used for training and sample 0 and 2 (remember, Python uses 0-based indexing!) should be used for testing.\n",
    "\n",
    "Now, we know how to access the train- and test-indices, but we haven't *actually* indexed our X and y (i.e. mvp.X and mvp.y) in the for-loop over folds. Nor have we actually fit the model on the train-set and cross-validated this to the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: in the code-cell below, complete the statements by indexing mvp.X and mvp.y to create four different objects in every fold: X_train, y_train, X_test, y_test. Also, we created a new classifier-object (clf) for you based on a different model: scikit-learn's `LogisticRegression` to show you that *every* model in scikit-learn works the same. Use this classifier to fit on the train-set and predict the test-set in every fold. Then, calculate the (cross-validated) accuracy in every fold. Keep track of the accuracies across folds, and after the loop over folds, calculate the average accuracy across folds.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf now is a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "# run split() again to generate folds\n",
    "folds = skf.split(mvp.X, mvp.y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    # This is a slightly more concise way to unpack tuples\n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # Complete these statements by indexing mvp.X and mvp.y with train_idx and test_idx\n",
    "    X_train = \n",
    "    y_train = \n",
    "    \n",
    "    X_test = \n",
    "    y_test = \n",
    "    \n",
    "    # ToDo: call fit (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-fold cross-validation instead of hold-out cross-validation allowed us to make our analysis a little more efficient (by reusing samples). Another method that (usually) improves performance in decoding analyses is feature selection/extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection/extraction\n",
    "As discussed before, a small sample/feature-ratio often results in overfitting (optimistic performance estimates on train-set). It follows then that reducing the amount of features often has a beneficial effect on cross-validated performance estimates.\n",
    "\n",
    "Feature reduction can be achieved in two principled ways:\n",
    "\n",
    "* feature extraction: transform your features into a set of lower-dimensional components;\n",
    "* feature selection: select a subset of features\n",
    "\n",
    "Examples of feature extraction are PCA (i.e. transform voxels to orthogonal components) and averaging features within brain regions from an atlas. Because feature extraction often does not use any information from the labels (`y`), this step does not need to be cross-validated (but you *can* cross-validate, e.g., PCA by estimating the components on the train-set only). \n",
    "\n",
    "Examples of feature selection are ROI-analysis (i.e. restricting your patterns to a specific ROI in the brain) and \"univariate feature selection\" (UFS). This latter method is an often-used data-driven method to select features based upon their univariate difference. \n",
    "\n",
    "Fortunately, scikit-learn has a bunch of feature selection/extraction objects for us to use. These objects (\"transformers\" in scikit-learn lingo) work similarly to estimators: they also have a `fit(X, y)` method, in which for example the univariate differences (in UFS) or PCA-components (in PCA-driven feature extraction) are computed. Then, instead of having a `predict(X)` method, transformers have a `tranform(X)` method.\n",
    "\n",
    "Let's take a closer look at how UFS can be implemented using scikit-learn. Basically, it follows the following structure:\n",
    "\n",
    "`transformer = Selector(score_func=ufs_function, other_args)`\n",
    "\n",
    "Here, the `score_func` parameter takes a function that implements a univariate statistical test (like an [f-test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) or a [chi-square test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)). The `Selector` class determines how to select the subset of features based on the result of the `score_func`. \n",
    "\n",
    "For example, the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) selector selects the best `K` features based on their scores from the statistical test. For example, the following transformer would select the best 100 features based on their F-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# f_classif is a scikit-learn specific implementation of the F-test\n",
    "select100best = SelectKBest(score_func=f_classif, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a selector is `SelectFwe` which selects only features with statistics-values corresponding to p-values lower than a set alpha-level. For example, the following transformer would only select features with p-values from a chi-square test lower than 0.01: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "selectfwe_transformer = SelectFwe(score_func=chi2, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: The functions `f_classif` and `chi2` are functions to calculate statistics for when the the DV is categorical (i.e. in classification scenarios). Can you think of a function that does the same when your DV is continuous (i.e. in regression scenarios)? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does this work in practice? We'll show you an (not cross-validated!) example using the select100best transformer initialized earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the transformer ...\n",
    "select100best.fit(mvp.X, mvp.y)\n",
    "\n",
    "# ... which calculates the following attributes (.scores_ and .pvalues_)\n",
    "# Let's check them out\n",
    "scores, pvalues = select100best.scores_, select100best.pvalues_\n",
    "\n",
    "# As you can see, each voxel gets its own score (in this case: an F-score)\n",
    "print(scores.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize these scores in brain space (it's a coronal slice sideways ...): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scores_3d = scores.reshape((91, 109, 91))\n",
    "plt.imshow(scores_3d[:, 75, :], origin='lower', cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in the image above, brighter (more yellow) colors represent voxels with higher scores on the univariate test. If we subsequently apply (or \"transform\" in scikit-learn lingo) our X-matrix using, for example, the `select100best` selector, we'll select only the 100 \"most yellow\" (i.e. highest F-scores) voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: Given the image above, what is the a major difference between data driven feature selection (like UFS) and ROI-based feature selection (e.g. only look at patterns in the amygdala) in terms of the spatial scale of patterns you'll select? Try to think of an example of UFS over ROI-based feature selection and vice versa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically, after we've fit the transformer, we can call the `transform(X)` method that will actually select the subset according to the selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape of mvp.X before transform: %r ...\" % (mvp.X.shape,))\n",
    "X_after_ufs = select100best.transform(mvp.X)\n",
    "\n",
    "print(\"... and shape of mvp.X after transform: %r.\" % (X_after_ufs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the transformer correctly selected a subset of 100 voxels from our mvp.X matrix! Now, both selectors were fit on the entire dataset, which is often course not how it should be done: because they use information from the labels (`y`), this step should be cross-validated. Thus, what you have to do is:\n",
    "\n",
    "* fit your transformer on the train-set;\n",
    "* transform your train-set;\n",
    "* transform your test-set;\n",
    "* (fit your model on the train-set;)\n",
    "* (cross-validate to the test-set;)\n",
    "\n",
    "We summarized the entire pipeline, including data partitioning (indexing), feature selection (transformation), and model fitting and their corresponding cross-validation steps in the image below:\n",
    "<img src='sklearn_transformers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the assignment, implement a cross-validated pipeline with feature selection *and* model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 2** (2 points): Below, we set up a K-fold cross-validation loop and prespecified a classifier (`clf`, a logistic regression model) and a transformer (`select50best`, selecting the 50 best features based upon an F-test). Now, it's up to you to actually implement the feature selection inside the for-loop. Make sure to fit the transformer only on the train-set, but then transform *both* the train-set and the test-set. Then, fit the model on the transformed train-set and cross-validate to the transformed test-set. Calculate accuracy of the cross-validated  model for each fold, and after all folds calculate the average accuracy (across folds).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "folds = skf.split(mvp.X, mvp.y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "select50best = SelectKBest(score_func=f_classif, k=50)\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # ToDo: make X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # ToDo: call the select50best fit method (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: advanced cross-validation using Pipelines\n",
    "As you have seen in the previous assignment, the code within the K-fold for-loop becomes quite long (and messy) when you add a feature-selection step. Suppose you want to add another feature extraction step, like performing a PCA after an initial feature selection step. The code becomes even more obscure ...\n",
    "\n",
    "Luckily, scikit-learn has an awesome solution for this: \"Pipelines\".\n",
    "\n",
    "Pipelines are somewhat more advanced functionality within scikit-learn, but we wanted to show you guys because it really \"cleans up\" your code, and additionally, it is a great safeguard for accidental overfitting (e.g. when you accidentally perform feature-selection on all your data instead of only your train-data). \n",
    "\n",
    "Anyway, how does this work? Well, a picture it worth a thousand words, so check out the image below which schematically depicts what a pipeline does: <img src='pipelinesX.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a `Pipeline`-object \"strings together\" an arbitrary number of transformers (including \"preprocessing\" transformers like `StandardScaler`\\*) and can optionally end in an estimator. Pipeline-objects have two (relevant) methods: `fit(X, y)` and `predict(X, y)` (the latter method only exists if there is an estimator in the pipeline). To use it, you only have to call `fit()` with `X_train` and `y_train` and it'll sequentially fit the transformers which will finally pass the transformed data to the estimator. Then, simply call `predict()` with `X_test` as argument and the pipeline will automatically cross-validate all fitted transformers, and eventually the estimator, on the `X_test` variable. \n",
    "\n",
    "Okay, lot's of info - let's break this down. First, let's initialize some transformers and an estimator:\n",
    "\n",
    "------------\n",
    "\\* [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is a scikit-learn transformer that does exactly the same as the `standardize()` method you implemented earlier! Strictly speaking, you don't need to cross-validate your standardization step, but it doesn't hurt either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ufs = SelectKBest(score_func=f_classif, k=1000)\n",
    "pca = PCA(n_components=10)  # we want to reduce the features to 10 components\n",
    "svc = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to initialize a Pipeline-object, we need to give it a list of tuples, in the first entry of each tuple is a name for the step in the pipeline and the second entry of each tuple is the actual transformer/estimator object. Let's do that for our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_to_make = [('preproc', scaler),\n",
    "                    ('ufs', ufs),\n",
    "                    ('pca', pca),\n",
    "                    ('clf', svc)]\n",
    "\n",
    "my_pipe = Pipeline(pipeline_to_make)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our pipeline-object (`my_pipe`) on our data. For this example, we'll use a simple hold-out cross-validation scheme (but pipelines are equally valuable in K-fold CV schemes!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = mvp.X[0::2], mvp.y[0::2]\n",
    "X_test, y_test = mvp.X[1::2], mvp.y[1::2]\n",
    "\n",
    "my_pipe.fit(X_train, y_train)\n",
    "predictions = my_pipe.predict(X_test)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(\"Cross-validated accuracy on test-set: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool stuff huh? Quite efficient, I would say! Anyway, let's continue with the last part of this tutorial: permutation testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Permutation testing\n",
    "While group-level statistics in within-subject decoding analyses are easily implemented (usually with a t-test of subject-wise accuracy scores against chance level), between-subject decoding analyses are trickier. That is because the data-points in between-subject analyses - usually the accuracy scores associated with different folds in K-fold CV - are *not independent* - which is an important assumption of all parametric statistics (such as t-tests). \n",
    "\n",
    "As explained in the lecture, dependence between data-points is created because the same samples may be included in different folds. This may not immediately make sense, but imagine the following: suppose we have 100 samples. For some reason, there is one sample that is *suuuuper* noisy. In fact, everytime it is included in the train-set (90%, so 90 trials), it messes up the fit of the model so badly, that it always leads to chance-level predictions on the test-set (on average only 5 samples in the test set are predicted correctly). When this noisy sample is included in the test-set, however, it doesn't mess up the model fit, and on average the test-set is predicted correctly with 80% accuracy. \n",
    "\n",
    "As you can see, this super noisy sample actually creates dependence between the accuracies associated with the folds in which it has been part of the train-set: these accuracies are *all* relatively lower than the folds in which this sample was part of the test-set. \n",
    "\n",
    "Anyway, long story short: parametric statistics don't work, so let's see how to implement a permutation test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Getting your *observed* performance score\n",
    "Before you start with anything related to permutation testing, you first need to get your performance score that you would like to get a p-values for. In between-subject decoding analyses, this is usually the average accuracy (or whatever other metric) across folds. So, when you would implement a 10-fold cross-validation scheme, your observed performance score would be the average of the accuracy-estimates across those 10 folds. You know what? Let's implement exactly that.\n",
    "\n",
    "This time, however, we won't do any data-drive feature selection, but we'll implement a ROI-based decoding analysis in which we only use the patterns in the left amygdala. This choice for the left amygdala is not theoretically motivated (the amygdala is unlikely to contain any information during a working memory task that is predictive for intelligence ... but you never know...), but is rather a practical concern: permutation testing takes a loooooot of time, and by limiting ourselves to a small subset of features (i.e. voxels in the left amygdala), we substantially speed up the (permutation) analysis.\n",
    "\n",
    "Anyway, let's implement such an analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mvp.apply_mask('Left_Amygdala_mask.nii.gz', threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Apply amygdala mask\n",
    "print(\"Shape of mvp.X: %r\" % (mvp.X.shape,))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# We'll pick seven folds\n",
    "n_folds = 2\n",
    "skf = StratifiedKFold(n_splits=n_folds)\n",
    "folds = skf.split(mvp.X, mvp.y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    train_idx, test_idx = fold\n",
    "    X_train, X_test = mvp.X[train_idx], mvp.X[test_idx]\n",
    "    y_train, y_test = mvp.y[train_idx], mvp.y[test_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy.append(np.mean(pred == y_test))\n",
    "    \n",
    "observed_acc = np.mean(accuracy)\n",
    "print(\"Mean accuracy across folds = %.3f\" % observed_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, not bad! 52.9% correct is higher than chance, but is it also *significantly* higher than chance? For that, we need to \"simulate\" a null-distribution through permutation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Getting your *permuted* performance distribution\n",
    "The goal of permutation tests are to create a null-distribution of your observed measure, in this case: (average) classification accuracy. The null-distribution refers to the distribution of classification accuracies that would arise if you would repeat an experiment with noisy data in which there is no effect (i.e. the null-hypothesis is true). \n",
    "\n",
    "Thus, we need to somehow repeat the above \"experiment\" (i.e. the analysis which yielded the observed performance of 52.9%) yet while the null-hypothesis is true (i.e. there is no effect, only noise). Well, as the term \"permutation\" suggest, we can simply randomly shuffly the train-labels (`y_train`) to generate random labels. Now, if we would fit the classifier on {`X_train`, and `y_train`}, then practically it would fit on noise. In fact, by random shuffling of the train-labels, we simulated the scenario in which (on average) the null-hypothesis would be true.\n",
    "\n",
    "So, what we need to do in order to create a null-distribution of classification-accuracies is to run our original analysis (in the code cell above) many times (i.e. \"repeat the experiment\"), yet with random labeling of our train-samples. We expect that the mean of these null-accuracies would center around .5 (assumed chance level), but simply due to random noise, our simulated null-distribution will contain values (sometimes substantially) above and below chance level. \n",
    "\n",
    "Anyway, look at the code-cell below. It contains exactly the same analysis as in the above code-block, yet now it is looped 1000 times, and just before fitting (within every fold separately) we shuffle the train-labels to generate a random mapping between `X_train` and `y_train`. After every \"experiment\" (or simply \"permutation\") we average the permuted accuracy scores across folds and store them (in the variable `permuted_accuracies`). Now, let's run the code cell below (may take minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_permutations = 1000\n",
    "\n",
    "permuted_accuracies = np.zeros(n_permutations)\n",
    "for i in range(n_permutations):\n",
    "\n",
    "    folds = skf.split(mvp.X, mvp.y)\n",
    "    fold_accuracies = np.zeros(n_folds)\n",
    "\n",
    "    for ii, fold in enumerate(folds):\n",
    "    \n",
    "        train_idx, test_idx = fold\n",
    "        X_train, X_test = mvp.X[train_idx], mvp.X[test_idx]\n",
    "        y_train, y_test = mvp.y[train_idx], mvp.y[test_idx]\n",
    "    \n",
    "        # Here we shuffle the y-train labels!!!\n",
    "        np.random.shuffle(y_train)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        fold_accuracies[ii] = np.mean(pred == y_test)\n",
    "    \n",
    "    permuted_accuracies[i] = np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we now got our simulated null-distribution values stored in the variable `permuted_accuracies`. Let's look at how the distribution actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Permuted null-distribution')\n",
    "plt.hist(permuted_accuracies, bins=25)\n",
    "plt.xlabel('Average accuracy across folds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember the 'formula' for the p-value in a permutation test? Given *N* permutations, it's the following (in pseudo-notation):\n",
    "\n",
    "\\begin{align}\n",
    "p = \\frac{1}{N}\\sum_{i=1}^{N}{permuted_{i} > observed}\n",
    "\\end{align}\n",
    "\n",
    "Put differently: the p-value expresses the proportion of permutation-values are higher than the observed value. Graphically, we can visualize the p-value as the area of the null-distribution right of the dotted red line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Permuted null-distribution')\n",
    "plt.hist(permuted_accuracies, bins=25)\n",
    "plt.xlabel('Average accuracy across folds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(observed_acc, c='r', linestyle='dashed', linewidth=3)\n",
    "plt.legend(['Observed accuracy'], loc=2, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "**Assignment 3** (1 point): calculate the p-value corresponding to the observed accuracy given our permuted null-distribution. Also, interpret the result: what does this tell us about our research question? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate p-value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
